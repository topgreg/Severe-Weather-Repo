{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a4102f31",
   "metadata": {},
   "source": [
    "# Severe Weather Capstone - Data Collection & Wrangling\n",
    "\n",
    "Greg Welliver   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ba529c28",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import relevant libraries and packages.\n",
    "import numpy as np \n",
    "import pandas as pd \n",
    "import matplotlib.pyplot as plt \n",
    "import seaborn as sns \n",
    "import statsmodels.api as sm\n",
    "\n",
    "from statsmodels.graphics.api import abline_plot\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import linear_model, preprocessing \n",
    "import warnings\n",
    "from scipy import stats\n",
    "import re\n",
    "from glob import glob, iglob\n",
    "from datetime import datetime\n",
    "import string\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "66d847d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#NotebookApp.iopub_data_rate_limit=1.0e10\n",
    "#https://stackoverflow.com/questions/43288550/iopub-data-rate-exceeded-in-jupyter-notebook-when-viewing-image"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d44e420",
   "metadata": {},
   "source": [
    "file location for downloads: \n",
    "    https://www.ncei.noaa.gov/pub/data/swdi/stormevents/csvfiles/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbf95157",
   "metadata": {},
   "source": [
    "## Data Collection\n",
    "\n",
    "- storm files were collected from the Iowa Environmental Mesonet: https://mesonet.agron.iastate.edu/nws/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "917fde8d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There is a total of 5 files.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/rc/20gvvbw10mg1xrcm82p5g4tm0000gn/T/ipykernel_16605/1426410123.py:13: DtypeWarning: Columns (15,28) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  storm_data = [pd.read_csv(filepath) for filepath in filenames]\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'hot100_all' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[0;32mIn [3]\u001b[0m, in \u001b[0;36m<cell line: 8>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m      9\u001b[0m     \u001b[38;5;66;03m# Read in Summary File is exists\u001b[39;00m\n\u001b[0;32m---> 10\u001b[0m     all_storm_data \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_csv\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtarget_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     11\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m:\n\u001b[1;32m     12\u001b[0m     \u001b[38;5;66;03m# Read in all Subfiles\u001b[39;00m\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/pandas/util/_decorators.py:311\u001b[0m, in \u001b[0;36mdeprecate_nonkeyword_arguments.<locals>.decorate.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    306\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[1;32m    307\u001b[0m         msg\u001b[38;5;241m.\u001b[39mformat(arguments\u001b[38;5;241m=\u001b[39marguments),\n\u001b[1;32m    308\u001b[0m         \u001b[38;5;167;01mFutureWarning\u001b[39;00m,\n\u001b[1;32m    309\u001b[0m         stacklevel\u001b[38;5;241m=\u001b[39mstacklevel,\n\u001b[1;32m    310\u001b[0m     )\n\u001b[0;32m--> 311\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/pandas/io/parsers/readers.py:678\u001b[0m, in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, error_bad_lines, warn_bad_lines, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options)\u001b[0m\n\u001b[1;32m    676\u001b[0m kwds\u001b[38;5;241m.\u001b[39mupdate(kwds_defaults)\n\u001b[0;32m--> 678\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_read\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/pandas/io/parsers/readers.py:575\u001b[0m, in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    574\u001b[0m \u001b[38;5;66;03m# Create the parser.\u001b[39;00m\n\u001b[0;32m--> 575\u001b[0m parser \u001b[38;5;241m=\u001b[39m \u001b[43mTextFileReader\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    577\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m chunksize \u001b[38;5;129;01mor\u001b[39;00m iterator:\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/pandas/io/parsers/readers.py:932\u001b[0m, in \u001b[0;36mTextFileReader.__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m    931\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles: IOHandles \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m--> 932\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_engine \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_make_engine\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mengine\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/pandas/io/parsers/readers.py:1216\u001b[0m, in \u001b[0;36mTextFileReader._make_engine\u001b[0;34m(self, f, engine)\u001b[0m\n\u001b[1;32m   1213\u001b[0m \u001b[38;5;66;03m# error: No overload variant of \"get_handle\" matches argument types\u001b[39;00m\n\u001b[1;32m   1214\u001b[0m \u001b[38;5;66;03m# \"Union[str, PathLike[str], ReadCsvBuffer[bytes], ReadCsvBuffer[str]]\"\u001b[39;00m\n\u001b[1;32m   1215\u001b[0m \u001b[38;5;66;03m# , \"str\", \"bool\", \"Any\", \"Any\", \"Any\", \"Any\", \"Any\"\u001b[39;00m\n\u001b[0;32m-> 1216\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;241m=\u001b[39m \u001b[43mget_handle\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# type: ignore[call-overload]\u001b[39;49;00m\n\u001b[1;32m   1217\u001b[0m \u001b[43m    \u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1218\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1219\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mencoding\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1220\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcompression\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcompression\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1221\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmemory_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmemory_map\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1222\u001b[0m \u001b[43m    \u001b[49m\u001b[43mis_text\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_text\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1223\u001b[0m \u001b[43m    \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mencoding_errors\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstrict\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1224\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstorage_options\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1225\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1226\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/pandas/io/common.py:786\u001b[0m, in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    784\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mencoding \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mmode:\n\u001b[1;32m    785\u001b[0m     \u001b[38;5;66;03m# Encoding\u001b[39;00m\n\u001b[0;32m--> 786\u001b[0m     handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[1;32m    787\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhandle\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    788\u001b[0m \u001b[43m        \u001b[49m\u001b[43mioargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    789\u001b[0m \u001b[43m        \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mioargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencoding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    790\u001b[0m \u001b[43m        \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43merrors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    791\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnewline\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    792\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    793\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    794\u001b[0m     \u001b[38;5;66;03m# Binary mode\u001b[39;00m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '../Data/all_storm_data.csv'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[0;32mIn [3]\u001b[0m, in \u001b[0;36m<cell line: 8>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     14\u001b[0m     all_storm_data \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mconcat(storm_data)\n\u001b[1;32m     16\u001b[0m     \u001b[38;5;66;03m# Create Summary File for faster processing\u001b[39;00m\n\u001b[0;32m---> 17\u001b[0m     \u001b[43mhot100_all\u001b[49m\u001b[38;5;241m.\u001b[39mto_csv(target_path,index\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m     19\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe total number of observations is \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\u001b[38;5;28mlen\u001b[39m(all_storm_data)))\n\u001b[1;32m     20\u001b[0m all_storm_data\u001b[38;5;241m.\u001b[39mhead()\n",
      "\u001b[0;31mNameError\u001b[0m: name 'hot100_all' is not defined"
     ]
    }
   ],
   "source": [
    "### working code, make markdown for now\n",
    "### All annual storm data files are saved on my local machine.  This code gathers all of the files and combines them into one file.\n",
    "filenames = glob('../Data/*.csv')\n",
    "print(\"There is a total of {} files.\".format(len(filenames)))\n",
    "\n",
    "target_path = '../Data/all_storm_data.csv'\n",
    "\n",
    "try:\n",
    "    # Read in Summary File is exists\n",
    "    all_storm_data = pd.read_csv(target_path)\n",
    "except:\n",
    "    # Read in all Subfiles\n",
    "    storm_data = [pd.read_csv(filepath) for filepath in filenames]\n",
    "    all_storm_data = pd.concat(storm_data)\n",
    "    \n",
    "    # Create Summary File for faster processing\n",
    "    hot100_all.to_csv(target_path,index=False)\n",
    "\n",
    "print(\"The total number of observations is {}.\".format(len(all_storm_data)))\n",
    "all_storm_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "892bc82a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load data\n",
    "#df = pd.read_csv(\"../Data/StormEvents_details-ftp_v1.0_d2001_c20220425.csv\")\n",
    "#df = pd.read_parquet(\"../Data/all_storm_data.pqt\")\n",
    "#df = pd.read_csv(\"../Data/all_storm_data4.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1b3f0a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bb2c033",
   "metadata": {},
   "outputs": [],
   "source": [
    "#df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b4bb053",
   "metadata": {},
   "outputs": [],
   "source": [
    "#df.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "451201e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16c16bdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "for row in df[\"STATE_FIPS\"][:10]:\n",
    "    res = row.split(\".\", 1)[0]\n",
    "    print(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5efd0224",
   "metadata": {},
   "outputs": [],
   "source": [
    "# WORKING, MARKDOWN UNTIL FINAL\n",
    "# drop unnecessary columns\n",
    "df.drop(['CATEGORY', 'DATA_SOURCE', 'BEGIN_RANGE', 'BEGIN_AZIMUTH', 'BEGIN_LOCATION', 'END_RANGE', 'END_AZIMUTH', 'END_LOCATION', 'TOR_OTHER_WFO', 'TOR_OTHER_CZ_STATE', 'TOR_OTHER_CZ_FIPS', 'TOR_OTHER_CZ_NAME', 'CZ_TIMEZONE', 'WFO', 'CZ_TYPE', 'DAMAGE_CROPS', 'CZ_NAME', 'SOURCE', 'BEGIN_DAY', 'END_YEARMONTH', 'END_DAY', 'END_TIME', 'EPISODE_ID', 'EVENT_ID', 'STATE_FIPS', 'CZ_FIPS', 'END_DATE_TIME'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fabe25ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# WORKING, MARKDOWN UNTIL FINAL\n",
    "df['STATE_FIPS'] = df['STATE_FIPS'].astype(object)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14418bb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# WORKING, MARKDOWN UNTIL FINAL\n",
    "# Columns to replace nulls with NA:\n",
    "cols_na = ['EVENT_NARRATIVE', 'EPISODE_NARRATIVE', 'BEGIN_LAT', 'BEGIN_LON', 'END_LAT', 'END_LON', 'TOR_F_SCALE', 'MAGNITUDE_TYPE', 'FLOOD_CAUSE', 'STATE', 'STATE_FIPS']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f576531f",
   "metadata": {},
   "outputs": [],
   "source": [
    "for x in cols_na:\n",
    "    print(df[x].isna().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90e01e90",
   "metadata": {},
   "outputs": [],
   "source": [
    "# WORKING, MARKDOWN UNTIL FINAL\n",
    "for x in cols_na:\n",
    "    df[x] = df[x].fillna('NA')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce9f17db",
   "metadata": {},
   "outputs": [],
   "source": [
    "for x in cols_na:\n",
    "    print(df[x].isna().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "225c8d63",
   "metadata": {},
   "outputs": [],
   "source": [
    "# WORKING, MARKDOWN UNTIL FINAL\n",
    "# Columns to replace nulls with 0:\n",
    "\n",
    "cols_0 = ['MAGNITUDE', 'TOR_LENGTH', 'TOR_WIDTH', 'DAMAGE_PROPERTY', 'INJURIES_DIRECT', 'INJURIES_INDIRECT', 'DEATHS_DIRECT', 'DEATHS_INDIRECT']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be86aaaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "for x in cols_0:\n",
    "    print(df[x].isna().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb14100f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# WORKING, MARKDOWN UNTIL FINAL\n",
    "for x in cols_0:\n",
    "    df[x] = df[x].fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f6b860d",
   "metadata": {},
   "outputs": [],
   "source": [
    "for x in cols_0:\n",
    "    print(df[x].isna().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dea7af28",
   "metadata": {},
   "outputs": [],
   "source": [
    "#df.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79757668",
   "metadata": {},
   "outputs": [],
   "source": [
    "#df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d150897",
   "metadata": {},
   "outputs": [],
   "source": [
    "# WORKING, MARKDOWN UNTIL FINAL\n",
    "# remove NA values from state FIPS\n",
    "df = df[df['STATE_FIPS'] != \"NA\"].reset_index()\n",
    "\n",
    "# convert STATE_FIPS to INT so can use it for lookup later\n",
    "for x in df['STATE_FIPS']:\n",
    "    x = int(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d880776",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['STATE_FIPS'] = df['STATE_FIPS'].astype(int)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c82f7a79",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['STATE_FIPS'] = df['STATE_FIPS'].astype(object)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fbee281",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['STATE_FIPS']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc039d09",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['CZ_FIPS'] = df['CZ_FIPS'].astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "256ca765",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['CZ_FIPS']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4c054ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['CZ_FIPS'][:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "179e9f83",
   "metadata": {},
   "outputs": [],
   "source": [
    "#df[249750:249760]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5827357",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['CZ_FIPS'][249750:249760]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c7910d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# WORKING, MARKDOWN UNTIL FINAL\n",
    "# add \"0\" or \"00\" to CZ FIPS so that it can be used to match later\n",
    "for i in (range(len(df['CZ_FIPS']))):\n",
    "    if len(df['CZ_FIPS'][i]) == 2:\n",
    "#        df['CZ_FIPS'][i] = df['CZ_FIPS'][i].astype(str)\n",
    "        df['CZ_FIPS'][i] = \"0\" + df['CZ_FIPS'][i]\n",
    "#        print(df['CZ_FIPS'][i])\n",
    "    elif len(df['CZ_FIPS'][i]) == 1:\n",
    "        df['CZ_FIPS'][i] = \"00\" + df['CZ_FIPS'][i]\n",
    "#        print(df['CZ_FIPS'][i])\n",
    "#     else:\n",
    "#         row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6558e5fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['STATE_FIPS'] = df['STATE_FIPS'].astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c348f5ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# WORKING, MARKDOWN UNTIL FINAL\n",
    "# add \"0\" to state FIPS so that it can be used to match later\n",
    "for i in (range(len(df['STATE_FIPS']))):\n",
    "    if len(df['STATE_FIPS'][i]) == 1:\n",
    "#        df['CZ_FIPS'][i] = df['CZ_FIPS'][i].astype(str)\n",
    "        df['STATE_FIPS'][i] = \"0\" + df['STATE_FIPS'][i]\n",
    "#        print(df['CZ_FIPS'][i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1820db35",
   "metadata": {},
   "outputs": [],
   "source": [
    "#df[\"CZ_FIPS\"] = df.apply(lambda x: \"0\" + x if len(x) == 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e64aecf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# WORKING, MARKDOWN UNTIL FINAL\n",
    "# concatenate STATE FIPS and CZ FIPS into one column so that it can be used to match\n",
    "df['ST_CT_FIPS'] = df['STATE_FIPS'].astype(str) + df['CZ_FIPS'].astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bd575fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# WORKING, MARKDOWN UNTIL FINAL\n",
    "# remove all of the K's, M's, and B's in the DAMAGE_PROPERTY column and multiply them by appropriate values\n",
    "d = {r\"(\\d)K$\": r\"\\1*1000\", r\"M$\": r\"*1000000\", r\"B$\": r\"*1000000000\", r\"^K$\": r\"1000\"}\n",
    "\n",
    "#r stands for raw string\n",
    "#dollar is end of the line\n",
    "\n",
    "# for every key and value, run this code\n",
    "for k,v in d.items():\n",
    "     df[\"DAMAGE_PROPERTY\"] = df[\"DAMAGE_PROPERTY\"].str.replace(k, v, regex=True).fillna(\"0.0\")\n",
    "#df[\"DAMAGE_PROPERTY\"].apply(eval)\n",
    "df[\"DAMAGE_PROPERTY\"] = df[\"DAMAGE_PROPERTY\"].apply(eval)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5ab316d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# WORKING, MARKDOWN UNTIL FINAL\n",
    "# convert date strings to datetimes\n",
    "df['BEGIN_DATE_TIME'] =  pd.to_datetime(df['BEGIN_DATE_TIME'])\n",
    "df['END_DATE_TIME'] =  pd.to_datetime(df['END_DATE_TIME'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1755e51b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# WORKING, MARKDOWN UNTIL FINAL\n",
    "# calculate duration of storm\n",
    "df['DURATION'] = df['END_DATE_TIME'] - df['BEGIN_DATE_TIME']\n",
    "\n",
    "# convert storm duration to minutes\n",
    "for i in (range(len(df['DURATION']))):\n",
    "    df['DURATION'][i] = df['DURATION'][i].total_seconds() / 60"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8feab730",
   "metadata": {},
   "outputs": [],
   "source": [
    "# WORKING, MARKDOWN UNTIL FINAL\n",
    "# code to calculate coverage area of the storm\n",
    "\n",
    "# calculate  beginning and end latitude difference\n",
    "df['LAT_DIFF'] = (df['END_LAT'] - df['BEGIN_LAT']).abs()\n",
    "\n",
    "# calculate  beginning and end longitude difference\n",
    "df['LON_DIFF'] = (df['END_LON'] - df['END_LON']).abs()\n",
    "\n",
    "# combine two columns to calculate total size of storm\n",
    "df['STORM_AREA'] = df['LON_DIFF'] + df['LAT_DIFF']\n",
    "\n",
    "# since we don't need the difference columns anymore, drop those. also END LAT and LON columsn, since don't need those either\n",
    "df.drop(['LAT_DIFF', 'LON_DIFF', 'END_LON', 'END_LAT'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1b0a891",
   "metadata": {},
   "source": [
    "## Part 2 start here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97ba7134",
   "metadata": {},
   "outputs": [],
   "source": [
    "#load the data\n",
    "# df = pd.read_csv(\"../Data/all_storm_data4.csv\", index_col=[0])\n",
    "# df.drop(['index'], axis=1, inplace=True)\n",
    "df = pd.read_parquet(\"../Data/all_storm_data7.pqt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d686f1bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ff363f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head(20).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9694ca8",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['ST_CT_FIPS'] = df['ST_CT_FIPS'].astype(str)\n",
    "df['ST_CT_FIPS'] = df['ST_CT_FIPS'].str.zfill(5)\n",
    "df.ST_CT_FIPS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93e29f08",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b012683f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head().T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c961a653",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.ST_CT_FIPS.sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59e80566",
   "metadata": {},
   "source": [
    "## Combine Population Density, Home Price data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0696684a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#load the data\n",
    "#PopDen = pd.read_csv(\"../Data/Average_Household_Size_and_Population_Density_-_County_merge.csv\", index_col=[0])\n",
    "PopDen = pd.read_csv(\"../Data/Average_Household_Size_and_Population_Density_-_County_merge.csv\")\n",
    "#HomePrice = pd.read_csv(\"../Data/HPI_AT_BDL_county_merge.csv\", index_col=[0])\n",
    "#HomePrice = pd.read_csv(\"../Data/HPI_AT_BDL_county_merge.csv\")\n",
    "HomePrice = pd.read_excel(\"../Data/HPI_AT_BDL_county_merge.xlsx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32df271f",
   "metadata": {},
   "outputs": [],
   "source": [
    "PopDen = PopDen[PopDen['FIPS_CODE'].notnull()]\n",
    "PopDen = PopDen.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "959472d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# code to fix FIPS_CODE column in PopDen\n",
    "\n",
    "# convert to int to get rid of decimals\n",
    "PopDen['FIPS_CODE'] = PopDen['FIPS_CODE'].astype(int)\n",
    "\n",
    "# pad additional zeroes\n",
    "PopDen['FIPS_CODE'] = PopDen['FIPS_CODE'].astype(str)\n",
    "PopDen['FIPS_CODE'] = PopDen['FIPS_CODE'].str.zfill(5)\n",
    "\n",
    "# code to fix FIPS CODE column in HomePrice\n",
    "\n",
    "# pad additional zeroes\n",
    "HomePrice['FIPS code'] = HomePrice['FIPS code'].astype(str)\n",
    "HomePrice['FIPS code'] = HomePrice['FIPS code'].str.zfill(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1a77cb8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "PopDen.head(20).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84cf668c",
   "metadata": {},
   "outputs": [],
   "source": [
    "HomePrice.head(20).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef47ad50",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[df['ST_CT_FIPS'] == \"01001\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e9cb9dc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2589718d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "360ae852",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "5c0b2398",
   "metadata": {},
   "source": [
    "# working\n",
    "# merge the population density data to the main dataframe\n",
    "df = df.merge(PopDen['B01001_calc_PopDensity'], how = 'left',\n",
    "                left_on = 'ST_CT_FIPS', right_on = PopDen['FIPS_CODE'])\n",
    "#TopCountries.index = TopCountries.index + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ccb52f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# using pd.concat\n",
    "# merge the population density data to the main dataframe\n",
    "df = pd.concat([df, PopDen[['B01001_calc_PopDensity', 'Population']]],\n",
    "                  keys = ['ST_CT_FIPS', 'FIPS_CODE'])\n",
    "#TopCountries.index = TopCountries.index + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae32f16c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df.head(30).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47236998",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Population'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bd2c823",
   "metadata": {},
   "outputs": [],
   "source": [
    "### FIXING BELOW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f30bbd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[df['ST_CT_FIPS'] == \"01001\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e967b9e6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5e9f3ab",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6575ecbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### FIXING ABOVE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e6c937b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# merge the home price index data to the main dataframe\n",
    "df = pd.merge(df, HomePrice,  how='left', left_on=['ST_CT_FIPS','YEAR'], right_on = ['FIPS code','Year'])\n",
    "df.drop(['HPI with 2000 base', 'HPI with 1990 base', 'Annual Change (%)', 'Year', 'FIPS code', 'County', 'State',], axis=1, inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5b0a44f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head(30).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba0032d3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83d2d8c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad89c6ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afe271e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# code to replace \".\" so that we can ultimately turn this into a number\n",
    "\n",
    "new_list = []\n",
    "for i in df['HPI']:\n",
    "    a_string = str(i)\n",
    "    #a_string = df['HPI'][i]\n",
    "    new_string = a_string.translate(str.maketrans('', '', string.punctuation))\n",
    "    #print(a_string)\n",
    "    #df['new_HPI'][i] = new_string\n",
    "    #print(df['new_HPI'][i])\n",
    "    new_list.append(new_string)\n",
    "#    print(df['HPI'][i])    \n",
    "#     df['new_HPI'][i] = new_string\n",
    "#     print(df['new_HPI'][i])\n",
    "#print(new_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "039f568c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['newHPI'] = pd.DataFrame(new_list)\n",
    "df['newHPI'] = df['newHPI'].replace('nan', 'NaN')\n",
    "df['newHPI'] = df['newHPI'].replace('', 'NaN')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af06f0a4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bed595b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "9b2d2b26",
   "metadata": {},
   "source": [
    "# START HERE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c73c39a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_parquet(\"../Data/all_storm_data9.pqt\")\n",
    "#df = pd.read_parquet(\"../Data/all_storm_data9.pqt\", index_col=[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79f47f25",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a3ccbed",
   "metadata": {},
   "outputs": [],
   "source": [
    "#fixing the merge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "727a18b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head(20).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cdb4d01",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.PopDensity.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d636e94",
   "metadata": {},
   "outputs": [],
   "source": [
    "PopDen.head(20).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f92a925b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e38a3f3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0ae577c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b4b688d",
   "metadata": {},
   "outputs": [],
   "source": [
    "aaa"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "035bed96",
   "metadata": {},
   "source": [
    "#### write to CSV\n",
    "from pathlib import Path  \n",
    "filepath = Path('/Users/gregwelliver/Desktop/springboard_files/Severe-Weather-Repo/Data/all_storm_data5.csv')  \n",
    "filepath.parent.mkdir(parents=True, exist_ok=True)  \n",
    "df.to_csv(filepath)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55c98ff5",
   "metadata": {},
   "source": [
    "#### write to parquet\n",
    "parquet_file = 'example_pd.parquet'\n",
    "\n",
    "df.to_parquet(parquet_file, engine = 'pyarrow', compression = 'gzip')\n",
    "\n",
    "logging.info('Parquet file named \"%s\" has been written to disk', parquet_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b26e4c97",
   "metadata": {},
   "source": [
    "#### write to parquet\n",
    "from pathlib import Path  \n",
    "filepath = Path('/Users/gregwelliver/Desktop/springboard_files/Severe-Weather-Repo/Data/all_storm_data9.pqt')  \n",
    "filepath.parent.mkdir(parents=True, exist_ok=True)  \n",
    "df.to_parquet(filepath)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66275a93",
   "metadata": {},
   "source": [
    "resources"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "050aad81",
   "metadata": {},
   "source": [
    "CZ FIPS documentation: https://www.irsa.miami.edu/_assets/pdf/Documents/fips_statecounty_code.pdf\n",
    "\n",
    "Population density: https://covid19.census.gov/datasets/21843f238cbb46b08615fc53e19e0daf_1/explore?location=2.632620%2C0.315550%2C1.00\n",
    "\n",
    "Home price index: https://www.fhfa.gov/DataTools/Downloads/Pages/House-Price-Index-Datasets.aspx\n",
    "\n",
    "maybe useful: https://www.nar.realtor/research-and-statistics/housing-statistics/county-median-home-prices-and-monthly-mortgage-payment\n",
    "        \n",
    "land values: https://www.nass.usda.gov/Publications/Todays_Reports/reports/land0822.pdf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24d2a028",
   "metadata": {},
   "source": [
    "data that I created:\n",
    " - concatenated state and county codes for indentification\n",
    " - storm duration\n",
    " - storm area\n",
    " - county population density (pulled from other dataset)\n",
    " - land values (pulled from other dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fdb0786",
   "metadata": {},
   "source": [
    "drop columns\n",
    "# WORKING, MARKDOWN UNTIL FINAL\n",
    "# drop unnecessary columns\n",
    "df.drop(['', '', '', '',], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7da1f01c",
   "metadata": {},
   "source": [
    "# working\n",
    "# merge the home price index data to the main dataframe\n",
    "df = df.merge(HomePrice['HPI'], how = 'left',\n",
    "                left_on = 'ST_CT_FIPS', right_on = HomePrice['FIPS code'])\n",
    "#TopCountries.index = TopCountries.index + 1"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
