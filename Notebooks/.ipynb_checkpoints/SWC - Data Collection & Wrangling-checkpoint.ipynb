{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a4102f31",
   "metadata": {},
   "source": [
    "# Severe Weather Capstone - Data Collection & Wrangling\n",
    "\n",
    "Greg Welliver   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ba529c28",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import relevant libraries and packages.\n",
    "import numpy as np \n",
    "import pandas as pd \n",
    "import matplotlib.pyplot as plt \n",
    "import seaborn as sns \n",
    "import statsmodels.api as sm\n",
    "\n",
    "from statsmodels.graphics.api import abline_plot\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import linear_model, preprocessing \n",
    "import warnings\n",
    "from scipy import stats\n",
    "import re\n",
    "from glob import glob, iglob\n",
    "from datetime import datetime\n",
    "import string\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9b932c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#NotebookApp.iopub_data_rate_limit=1.0e10\n",
    "#https://stackoverflow.com/questions/43288550/iopub-data-rate-exceeded-in-jupyter-notebook-when-viewing-image"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d44e420",
   "metadata": {},
   "source": [
    "file location for downloads: \n",
    "    https://www.ncei.noaa.gov/pub/data/swdi/stormevents/csvfiles/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbf95157",
   "metadata": {},
   "source": [
    "## Data Collection\n",
    "\n",
    "- storm files were collected from the Iowa Environmental Mesonet: https://mesonet.agron.iastate.edu/nws/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "917fde8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "### working code, make markdown for now\n",
    "### All annual storm data files are saved on my local machine.  This code gathers all of the files and combines them into one file.\n",
    "filenames = glob('../Data/*.csv')\n",
    "print(\"There is a total of {} files.\".format(len(filenames)))\n",
    "\n",
    "target_path = '../Data/all_storm_data.csv'\n",
    "\n",
    "try:\n",
    "    # Read in Summary File is exists\n",
    "    all_storm_data = pd.read_csv(target_path)\n",
    "except:\n",
    "    # Read in all Subfiles\n",
    "    storm_data = [pd.read_csv(filepath) for filepath in filenames]\n",
    "    all_storm_data = pd.concat(storm_data)\n",
    "    \n",
    "    # Create Summary File for faster processing\n",
    "    hot100_all.to_csv(target_path,index=False)\n",
    "\n",
    "print(\"The total number of observations is {}.\".format(len(all_storm_data)))\n",
    "all_storm_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "892bc82a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load data\n",
    "#df = pd.read_csv(\"../Data/StormEvents_details-ftp_v1.0_d2001_c20220425.csv\")\n",
    "#df = pd.read_parquet(\"../Data/all_storm_data.pqt\")\n",
    "#df = pd.read_csv(\"../Data/all_storm_data4.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1b3f0a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bb2c033",
   "metadata": {},
   "outputs": [],
   "source": [
    "#df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b4bb053",
   "metadata": {},
   "outputs": [],
   "source": [
    "#df.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "451201e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16c16bdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "for row in df[\"STATE_FIPS\"][:10]:\n",
    "    res = row.split(\".\", 1)[0]\n",
    "    print(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5efd0224",
   "metadata": {},
   "outputs": [],
   "source": [
    "# WORKING, MARKDOWN UNTIL FINAL\n",
    "# drop unnecessary columns\n",
    "df.drop(['CATEGORY', 'DATA_SOURCE', 'BEGIN_RANGE', 'BEGIN_AZIMUTH', 'BEGIN_LOCATION', 'END_RANGE', 'END_AZIMUTH', 'END_LOCATION', 'TOR_OTHER_WFO', 'TOR_OTHER_CZ_STATE', 'TOR_OTHER_CZ_FIPS', 'TOR_OTHER_CZ_NAME', 'CZ_TIMEZONE', 'WFO', 'CZ_TYPE', 'DAMAGE_CROPS', 'CZ_NAME', 'SOURCE', 'BEGIN_DAY', 'END_YEARMONTH', 'END_DAY', 'END_TIME', 'EPISODE_ID', 'EVENT_ID', 'STATE_FIPS', 'CZ_FIPS', 'END_DATE_TIME'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fabe25ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# WORKING, MARKDOWN UNTIL FINAL\n",
    "df['STATE_FIPS'] = df['STATE_FIPS'].astype(object)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14418bb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# WORKING, MARKDOWN UNTIL FINAL\n",
    "# Columns to replace nulls with NA:\n",
    "cols_na = ['EVENT_NARRATIVE', 'EPISODE_NARRATIVE', 'BEGIN_LAT', 'BEGIN_LON', 'END_LAT', 'END_LON', 'TOR_F_SCALE', 'MAGNITUDE_TYPE', 'FLOOD_CAUSE', 'STATE', 'STATE_FIPS']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f576531f",
   "metadata": {},
   "outputs": [],
   "source": [
    "for x in cols_na:\n",
    "    print(df[x].isna().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90e01e90",
   "metadata": {},
   "outputs": [],
   "source": [
    "# WORKING, MARKDOWN UNTIL FINAL\n",
    "for x in cols_na:\n",
    "    df[x] = df[x].fillna('NA')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce9f17db",
   "metadata": {},
   "outputs": [],
   "source": [
    "for x in cols_na:\n",
    "    print(df[x].isna().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "225c8d63",
   "metadata": {},
   "outputs": [],
   "source": [
    "# WORKING, MARKDOWN UNTIL FINAL\n",
    "# Columns to replace nulls with 0:\n",
    "\n",
    "cols_0 = ['MAGNITUDE', 'TOR_LENGTH', 'TOR_WIDTH', 'DAMAGE_PROPERTY', 'INJURIES_DIRECT', 'INJURIES_INDIRECT', 'DEATHS_DIRECT', 'DEATHS_INDIRECT']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be86aaaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "for x in cols_0:\n",
    "    print(df[x].isna().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb14100f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# WORKING, MARKDOWN UNTIL FINAL\n",
    "for x in cols_0:\n",
    "    df[x] = df[x].fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f6b860d",
   "metadata": {},
   "outputs": [],
   "source": [
    "for x in cols_0:\n",
    "    print(df[x].isna().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dea7af28",
   "metadata": {},
   "outputs": [],
   "source": [
    "#df.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79757668",
   "metadata": {},
   "outputs": [],
   "source": [
    "#df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d150897",
   "metadata": {},
   "outputs": [],
   "source": [
    "# WORKING, MARKDOWN UNTIL FINAL\n",
    "# remove NA values from state FIPS\n",
    "df = df[df['STATE_FIPS'] != \"NA\"].reset_index()\n",
    "\n",
    "# convert STATE_FIPS to INT so can use it for lookup later\n",
    "for x in df['STATE_FIPS']:\n",
    "    x = int(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d880776",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['STATE_FIPS'] = df['STATE_FIPS'].astype(int)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c82f7a79",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['STATE_FIPS'] = df['STATE_FIPS'].astype(object)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fbee281",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['STATE_FIPS']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc039d09",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['CZ_FIPS'] = df['CZ_FIPS'].astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "256ca765",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['CZ_FIPS']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4c054ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['CZ_FIPS'][:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "179e9f83",
   "metadata": {},
   "outputs": [],
   "source": [
    "#df[249750:249760]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5827357",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['CZ_FIPS'][249750:249760]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c7910d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# WORKING, MARKDOWN UNTIL FINAL\n",
    "# add \"0\" or \"00\" to CZ FIPS so that it can be used to match later\n",
    "for i in (range(len(df['CZ_FIPS']))):\n",
    "    if len(df['CZ_FIPS'][i]) == 2:\n",
    "#        df['CZ_FIPS'][i] = df['CZ_FIPS'][i].astype(str)\n",
    "        df['CZ_FIPS'][i] = \"0\" + df['CZ_FIPS'][i]\n",
    "#        print(df['CZ_FIPS'][i])\n",
    "    elif len(df['CZ_FIPS'][i]) == 1:\n",
    "        df['CZ_FIPS'][i] = \"00\" + df['CZ_FIPS'][i]\n",
    "#        print(df['CZ_FIPS'][i])\n",
    "#     else:\n",
    "#         row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6558e5fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['STATE_FIPS'] = df['STATE_FIPS'].astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c348f5ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# WORKING, MARKDOWN UNTIL FINAL\n",
    "# add \"0\" to state FIPS so that it can be used to match later\n",
    "for i in (range(len(df['STATE_FIPS']))):\n",
    "    if len(df['STATE_FIPS'][i]) == 1:\n",
    "#        df['CZ_FIPS'][i] = df['CZ_FIPS'][i].astype(str)\n",
    "        df['STATE_FIPS'][i] = \"0\" + df['STATE_FIPS'][i]\n",
    "#        print(df['CZ_FIPS'][i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1820db35",
   "metadata": {},
   "outputs": [],
   "source": [
    "#df[\"CZ_FIPS\"] = df.apply(lambda x: \"0\" + x if len(x) == 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e64aecf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# WORKING, MARKDOWN UNTIL FINAL\n",
    "# concatenate STATE FIPS and CZ FIPS into one column so that it can be used to match\n",
    "df['ST_CT_FIPS'] = df['STATE_FIPS'].astype(str) + df['CZ_FIPS'].astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bd575fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# WORKING, MARKDOWN UNTIL FINAL\n",
    "# remove all of the K's, M's, and B's in the DAMAGE_PROPERTY column and multiply them by appropriate values\n",
    "d = {r\"(\\d)K$\": r\"\\1*1000\", r\"M$\": r\"*1000000\", r\"B$\": r\"*1000000000\", r\"^K$\": r\"1000\"}\n",
    "\n",
    "#r stands for raw string\n",
    "#dollar is end of the line\n",
    "\n",
    "# for every key and value, run this code\n",
    "for k,v in d.items():\n",
    "     df[\"DAMAGE_PROPERTY\"] = df[\"DAMAGE_PROPERTY\"].str.replace(k, v, regex=True).fillna(\"0.0\")\n",
    "#df[\"DAMAGE_PROPERTY\"].apply(eval)\n",
    "df[\"DAMAGE_PROPERTY\"] = df[\"DAMAGE_PROPERTY\"].apply(eval)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5ab316d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# WORKING, MARKDOWN UNTIL FINAL\n",
    "# convert date strings to datetimes\n",
    "df['BEGIN_DATE_TIME'] =  pd.to_datetime(df['BEGIN_DATE_TIME'])\n",
    "df['END_DATE_TIME'] =  pd.to_datetime(df['END_DATE_TIME'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1755e51b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# WORKING, MARKDOWN UNTIL FINAL\n",
    "# calculate duration of storm\n",
    "df['DURATION'] = df['END_DATE_TIME'] - df['BEGIN_DATE_TIME']\n",
    "\n",
    "# convert storm duration to minutes\n",
    "for i in (range(len(df['DURATION']))):\n",
    "    df['DURATION'][i] = df['DURATION'][i].total_seconds() / 60"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8feab730",
   "metadata": {},
   "outputs": [],
   "source": [
    "# WORKING, MARKDOWN UNTIL FINAL\n",
    "# code to calculate coverage area of the storm\n",
    "\n",
    "# calculate  beginning and end latitude difference\n",
    "df['LAT_DIFF'] = (df['END_LAT'] - df['BEGIN_LAT']).abs()\n",
    "\n",
    "# calculate  beginning and end longitude difference\n",
    "df['LON_DIFF'] = (df['END_LON'] - df['END_LON']).abs()\n",
    "\n",
    "# combine two columns to calculate total size of storm\n",
    "df['STORM_AREA'] = df['LON_DIFF'] + df['LAT_DIFF']\n",
    "\n",
    "# since we don't need the difference columns anymore, drop those. also END LAT and LON columsn, since don't need those either\n",
    "df.drop(['LAT_DIFF', 'LON_DIFF', 'END_LON', 'END_LAT'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1b0a891",
   "metadata": {},
   "source": [
    "## Part 2 start here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97ba7134",
   "metadata": {},
   "outputs": [],
   "source": [
    "#load the data\n",
    "# df = pd.read_csv(\"../Data/all_storm_data4.csv\", index_col=[0])\n",
    "# df.drop(['index'], axis=1, inplace=True)\n",
    "df = pd.read_parquet(\"../Data/all_storm_data7.pqt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d686f1bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ff363f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head().T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9694ca8",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['ST_CT_FIPS'] = df['ST_CT_FIPS'].astype(str)\n",
    "df['ST_CT_FIPS'] = df['ST_CT_FIPS'].str.zfill(5)\n",
    "df.ST_CT_FIPS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93e29f08",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b012683f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head().T"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59e80566",
   "metadata": {},
   "source": [
    "## Combine Population Density, Home Price data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0696684a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#load the data\n",
    "#PopDen = pd.read_csv(\"../Data/Average_Household_Size_and_Population_Density_-_County_merge.csv\", index_col=[0])\n",
    "PopDen = pd.read_csv(\"../Data/Average_Household_Size_and_Population_Density_-_County_merge.csv\")\n",
    "#HomePrice = pd.read_csv(\"../Data/HPI_AT_BDL_county_merge.csv\", index_col=[0])\n",
    "#HomePrice = pd.read_csv(\"../Data/HPI_AT_BDL_county_merge.csv\")\n",
    "HomePrice = pd.read_excel(\"../Data/HPI_AT_BDL_county_merge.xlsx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32df271f",
   "metadata": {},
   "outputs": [],
   "source": [
    "PopDen = PopDen[PopDen['FIPS_CODE'].notnull()]\n",
    "PopDen = PopDen.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "959472d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# code to fix FIPS_CODE column in PopDen\n",
    "\n",
    "# convert to int to get rid of decimals\n",
    "PopDen['FIPS_CODE'] = PopDen['FIPS_CODE'].astype(int)\n",
    "\n",
    "# pad additional zeroes\n",
    "PopDen['FIPS_CODE'] = PopDen['FIPS_CODE'].astype(str)\n",
    "PopDen['FIPS_CODE'] = PopDen['FIPS_CODE'].str.zfill(5)\n",
    "\n",
    "# code to fix FIPS CODE column in HomePrice\n",
    "\n",
    "# pad additional zeroes\n",
    "HomePrice['FIPS code'] = HomePrice['FIPS code'].astype(str)\n",
    "HomePrice['FIPS code'] = HomePrice['FIPS code'].str.zfill(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c0b2398",
   "metadata": {},
   "source": [
    "# working\n",
    "# merge the population density data to the main dataframe\n",
    "df = df.merge(PopDen['B01001_calc_PopDensity'], how = 'left',\n",
    "                left_on = 'ST_CT_FIPS', right_on = PopDen['FIPS_CODE'])\n",
    "#TopCountries.index = TopCountries.index + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ccb52f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# using pd.concat\n",
    "# merge the population density data to the main dataframe\n",
    "df = pd.concat([df, PopDen[['B01001_calc_PopDensity', 'Population']]],\n",
    "                  keys = ['ST_CT_FIPS', 'FIPS_CODE'])\n",
    "#TopCountries.index = TopCountries.index + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae32f16c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df.head(30).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47236998",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Population']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e6c937b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# merge the home price index data to the main dataframe\n",
    "df = pd.merge(df, HomePrice,  how='left', left_on=['ST_CT_FIPS','YEAR'], right_on = ['FIPS code','Year'])\n",
    "df.drop(['HPI with 2000 base', 'HPI with 1990 base', 'Annual Change (%)', 'Year', 'FIPS code', 'County', 'State',], axis=1, inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5b0a44f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head(30).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba0032d3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83d2d8c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad89c6ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6271d6a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# code to replace \".\" so that we can ultimately turn this into a number\n",
    "\n",
    "new_list = []\n",
    "for i in df['HPI']:\n",
    "    a_string = str(i)\n",
    "    #a_string = df['HPI'][i]\n",
    "    new_string = a_string.translate(str.maketrans('', '', string.punctuation))\n",
    "    #print(a_string)\n",
    "    #df['new_HPI'][i] = new_string\n",
    "    #print(df['new_HPI'][i])\n",
    "    new_list.append(new_string)\n",
    "#    print(df['HPI'][i])    \n",
    "#     df['new_HPI'][i] = new_string\n",
    "#     print(df['new_HPI'][i])\n",
    "#print(new_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d0fbd6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['newHPI'] = pd.DataFrame(new_list)\n",
    "df['newHPI'] = df['newHPI'].replace('nan', 'NaN')\n",
    "df['newHPI'] = df['newHPI'].replace('', 'NaN')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a62d4b5c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bed595b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e0528b36",
   "metadata": {},
   "source": [
    "# START HERE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd9f1799",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_pqt(\"../Data/all_storm_data9.pqt\", index_col=[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d82dea2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99b3b16f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "035bed96",
   "metadata": {},
   "source": [
    "#### write to CSV\n",
    "from pathlib import Path  \n",
    "filepath = Path('/Users/gregwelliver/Desktop/springboard_files/Severe-Weather-Repo/Data/all_storm_data5.csv')  \n",
    "filepath.parent.mkdir(parents=True, exist_ok=True)  \n",
    "df.to_csv(filepath)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55c98ff5",
   "metadata": {},
   "source": [
    "#### write to parquet\n",
    "parquet_file = 'example_pd.parquet'\n",
    "\n",
    "df.to_parquet(parquet_file, engine = 'pyarrow', compression = 'gzip')\n",
    "\n",
    "logging.info('Parquet file named \"%s\" has been written to disk', parquet_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69b561db",
   "metadata": {},
   "source": [
    "#### write to parquet\n",
    "from pathlib import Path  \n",
    "filepath = Path('/Users/gregwelliver/Desktop/springboard_files/Severe-Weather-Repo/Data/all_storm_data9.pqt')  \n",
    "filepath.parent.mkdir(parents=True, exist_ok=True)  \n",
    "df.to_parquet(filepath)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66275a93",
   "metadata": {},
   "source": [
    "resources"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "050aad81",
   "metadata": {},
   "source": [
    "CZ FIPS documentation: https://www.irsa.miami.edu/_assets/pdf/Documents/fips_statecounty_code.pdf\n",
    "\n",
    "Population density: https://covid19.census.gov/datasets/21843f238cbb46b08615fc53e19e0daf_1/explore?location=2.632620%2C0.315550%2C1.00\n",
    "\n",
    "Home price index: https://www.fhfa.gov/DataTools/Downloads/Pages/House-Price-Index-Datasets.aspx\n",
    "\n",
    "maybe useful: https://www.nar.realtor/research-and-statistics/housing-statistics/county-median-home-prices-and-monthly-mortgage-payment\n",
    "        \n",
    "land values: https://www.nass.usda.gov/Publications/Todays_Reports/reports/land0822.pdf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24d2a028",
   "metadata": {},
   "source": [
    "data that I created:\n",
    " - concatenated state and county codes for indentification\n",
    " - storm duration\n",
    " - storm area\n",
    " - county population density (pulled from other dataset)\n",
    " - land values (pulled from other dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fdb0786",
   "metadata": {},
   "source": [
    "drop columns\n",
    "# WORKING, MARKDOWN UNTIL FINAL\n",
    "# drop unnecessary columns\n",
    "df.drop(['', '', '', '',], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7da1f01c",
   "metadata": {},
   "source": [
    "# working\n",
    "# merge the home price index data to the main dataframe\n",
    "df = df.merge(HomePrice['HPI'], how = 'left',\n",
    "                left_on = 'ST_CT_FIPS', right_on = HomePrice['FIPS code'])\n",
    "#TopCountries.index = TopCountries.index + 1"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
