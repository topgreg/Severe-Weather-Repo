{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a4102f31",
   "metadata": {},
   "source": [
    "# Severe Weather - Data Collection & Wrangling\n",
    "\n",
    "Greg Welliver   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba529c28",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import relevant libraries and packages.\n",
    "import numpy as np \n",
    "import pandas as pd \n",
    "import matplotlib.pyplot as plt \n",
    "import seaborn as sns \n",
    "import statsmodels.api as sm\n",
    "\n",
    "from statsmodels.graphics.api import abline_plot\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import linear_model, preprocessing \n",
    "import warnings\n",
    "from scipy import stats\n",
    "import re\n",
    "from glob import glob, iglob\n",
    "from datetime import datetime\n",
    "import string\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbf95157",
   "metadata": {},
   "source": [
    "## Data Collection\n",
    "\n",
    "- storm files were collected from the Iowa Environmental Mesonet: https://mesonet.agron.iastate.edu/nws/\n",
    "- file location for downloads: \n",
    "    https://www.ncei.noaa.gov/pub/data/swdi/stormevents/csvfiles/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "917fde8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "### working code, make markdown for now\n",
    "### All annual storm data files are saved on my local machine.  This code gathers all of the files and combines them into one file.\n",
    "filenames = glob('../Data/*.csv')\n",
    "print(\"There is a total of {} files.\".format(len(filenames)))\n",
    "\n",
    "target_path = '../Data/all_storm_data.csv'\n",
    "\n",
    "try:\n",
    "    # Read in Summary File is exists\n",
    "    all_storm_data = pd.read_csv(target_path)\n",
    "except:\n",
    "    # Read in all Subfiles\n",
    "    storm_data = [pd.read_csv(filepath) for filepath in filenames]\n",
    "    all_storm_data = pd.concat(storm_data)\n",
    "    \n",
    "    # Create Summary File for faster processing\n",
    "    #hot100_all.to_csv(target_path,index=False)\n",
    "\n",
    "print(\"The total number of observations is {}.\".format(len(all_storm_data)))\n",
    "all_storm_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "892bc82a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load data\n",
    "#df = pd.read_csv(\"../Data/StormEvents_details-ftp_v1.0_d2001_c20220425.csv\")\n",
    "df = pd.read_parquet(\"../Data/all_storm_data.pqt\")\n",
    "#df = pd.read_csv(\"../Data/all_storm_data4.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0185e594",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05e6a21a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# code to split column\n",
    "for row in df[\"STATE_FIPS\"][:10]:\n",
    "    res = row.split(\".\", 1)[0]\n",
    "    print(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5efd0224",
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop unnecessary columns\n",
    "df.drop(['CATEGORY', 'DATA_SOURCE', 'BEGIN_RANGE', 'BEGIN_AZIMUTH', 'BEGIN_LOCATION', 'END_RANGE', 'END_AZIMUTH', 'END_LOCATION', 'TOR_OTHER_WFO', 'TOR_OTHER_CZ_STATE', 'TOR_OTHER_CZ_FIPS', 'TOR_OTHER_CZ_NAME', 'CZ_TIMEZONE', 'WFO', 'CZ_TYPE', 'DAMAGE_CROPS', 'CZ_NAME', 'SOURCE', 'BEGIN_DAY', 'END_YEARMONTH', 'END_DAY', 'END_TIME', 'EPISODE_ID', 'EVENT_ID', 'STATE_FIPS', 'CZ_FIPS', 'END_DATE_TIME'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8b358fb",
   "metadata": {},
   "source": [
    "#### Replace nulls in columns with NA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14418bb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Columns to replace nulls with NA:\n",
    "cols_na = ['EVENT_NARRATIVE', 'EPISODE_NARRATIVE', 'BEGIN_LAT', 'BEGIN_LON', 'END_LAT', 'END_LON', 'TOR_F_SCALE', 'MAGNITUDE_TYPE', 'FLOOD_CAUSE', 'STATE', 'STATE_FIPS']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f576531f",
   "metadata": {},
   "outputs": [],
   "source": [
    "for x in cols_na:\n",
    "    print(df[x].isna().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90e01e90",
   "metadata": {},
   "outputs": [],
   "source": [
    "# WORKING, MARKDOWN UNTIL FINAL\n",
    "for x in cols_na:\n",
    "    df[x] = df[x].fillna('NA')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce9f17db",
   "metadata": {},
   "outputs": [],
   "source": [
    "for x in cols_na:\n",
    "    print(df[x].isna().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "823f74ae",
   "metadata": {},
   "source": [
    "#### Replace nulls in columns with 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "225c8d63",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Columns to replace nulls with 0:\n",
    "cols_0 = ['MAGNITUDE', 'TOR_LENGTH', 'TOR_WIDTH', 'DAMAGE_PROPERTY', 'INJURIES_DIRECT', 'INJURIES_INDIRECT', 'DEATHS_DIRECT', 'DEATHS_INDIRECT']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be86aaaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "for x in cols_0:\n",
    "    print(df[x].isna().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb14100f",
   "metadata": {},
   "outputs": [],
   "source": [
    "for x in cols_0:\n",
    "    df[x] = df[x].fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f6b860d",
   "metadata": {},
   "outputs": [],
   "source": [
    "for x in cols_0:\n",
    "    print(df[x].isna().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d150897",
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove NA values from state FIPS\n",
    "df = df[df['STATE_FIPS'] != \"NA\"].reset_index()\n",
    "\n",
    "# convert STATE_FIPS to INT so can use it for lookup later\n",
    "for x in df['STATE_FIPS']:\n",
    "    x = int(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d880776",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['STATE_FIPS'] = df['STATE_FIPS'].astype(int)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c82f7a79",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['STATE_FIPS'] = df['STATE_FIPS'].astype(object)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc039d09",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['CZ_FIPS'] = df['CZ_FIPS'].astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c7910d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# add \"0\" or \"00\" to CZ FIPS so that it can be used to match later\n",
    "for i in (range(len(df['CZ_FIPS']))):\n",
    "    if len(df['CZ_FIPS'][i]) == 2:\n",
    "#        df['CZ_FIPS'][i] = df['CZ_FIPS'][i].astype(str)\n",
    "        df['CZ_FIPS'][i] = \"0\" + df['CZ_FIPS'][i]\n",
    "#        print(df['CZ_FIPS'][i])\n",
    "    elif len(df['CZ_FIPS'][i]) == 1:\n",
    "        df['CZ_FIPS'][i] = \"00\" + df['CZ_FIPS'][i]\n",
    "#        print(df['CZ_FIPS'][i])\n",
    "#     else:\n",
    "#         row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6558e5fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['STATE_FIPS'] = df['STATE_FIPS'].astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c348f5ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# add \"0\" to state FIPS so that it can be used to match later\n",
    "for i in (range(len(df['STATE_FIPS']))):\n",
    "    if len(df['STATE_FIPS'][i]) == 1:\n",
    "#        df['CZ_FIPS'][i] = df['CZ_FIPS'][i].astype(str)\n",
    "        df['STATE_FIPS'][i] = \"0\" + df['STATE_FIPS'][i]\n",
    "#        print(df['CZ_FIPS'][i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e64aecf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# concatenate STATE FIPS and CZ FIPS into one column so that it can be used to match\n",
    "df['ST_CT_FIPS'] = df['STATE_FIPS'].astype(str) + df['CZ_FIPS'].astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bd575fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove all of the K's, M's, and B's in the DAMAGE_PROPERTY column and multiply them by appropriate values\n",
    "d = {r\"(\\d)K$\": r\"\\1*1000\", r\"M$\": r\"*1000000\", r\"B$\": r\"*1000000000\", r\"^K$\": r\"1000\"}\n",
    "\n",
    "#r stands for raw string\n",
    "#dollar is end of the line\n",
    "\n",
    "# for every key and value, run this code\n",
    "for k,v in d.items():\n",
    "     df[\"DAMAGE_PROPERTY\"] = df[\"DAMAGE_PROPERTY\"].str.replace(k, v, regex=True).fillna(\"0.0\")\n",
    "#df[\"DAMAGE_PROPERTY\"].apply(eval)\n",
    "df[\"DAMAGE_PROPERTY\"] = df[\"DAMAGE_PROPERTY\"].apply(eval)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5ab316d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert date strings to datetimes\n",
    "df['BEGIN_DATE_TIME'] =  pd.to_datetime(df['BEGIN_DATE_TIME'])\n",
    "df['END_DATE_TIME'] =  pd.to_datetime(df['END_DATE_TIME'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1755e51b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate duration of storm\n",
    "df['DURATION'] = df['END_DATE_TIME'] - df['BEGIN_DATE_TIME']\n",
    "\n",
    "# convert storm duration to minutes\n",
    "for i in (range(len(df['DURATION']))):\n",
    "    df['DURATION'][i] = df['DURATION'][i].total_seconds() / 60"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8feab730",
   "metadata": {},
   "outputs": [],
   "source": [
    "# code to calculate coverage area of the storm\n",
    "\n",
    "# calculate  beginning and end latitude difference\n",
    "df['LAT_DIFF'] = (df['END_LAT'] - df['BEGIN_LAT']).abs()\n",
    "\n",
    "# calculate  beginning and end longitude difference\n",
    "df['LON_DIFF'] = (df['END_LON'] - df['END_LON']).abs()\n",
    "\n",
    "# combine two columns to calculate total size of storm\n",
    "df['STORM_AREA'] = df['LON_DIFF'] + df['LAT_DIFF']\n",
    "\n",
    "# since we don't need the difference columns anymore, drop those. also END LAT and LON columsn, since don't need those either\n",
    "df.drop(['LAT_DIFF', 'LON_DIFF', 'END_LON', 'END_LAT'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1b0a891",
   "metadata": {},
   "source": [
    "## Combine Population Density, Home Price data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97ba7134",
   "metadata": {},
   "outputs": [],
   "source": [
    "#load the data file in it's current state\n",
    "df = pd.read_parquet(\"../Data/all_storm_data7.pqt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d686f1bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ff363f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head(20).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9694ca8",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['ST_CT_FIPS'] = df['ST_CT_FIPS'].astype(str)\n",
    "df['ST_CT_FIPS'] = df['ST_CT_FIPS'].str.zfill(5)\n",
    "df.ST_CT_FIPS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93e29f08",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b012683f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head().T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c961a653",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.ST_CT_FIPS.sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0696684a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#load the data the other data files\n",
    "PopDen = pd.read_csv(\"../Data/Average_Household_Size_and_Population_Density_-_County_merge.csv\")\n",
    "HomePrice = pd.read_excel(\"../Data/HPI_AT_BDL_county_merge.xlsx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32df271f",
   "metadata": {},
   "outputs": [],
   "source": [
    "PopDen = PopDen[PopDen['FIPS_CODE'].notnull()]\n",
    "PopDen = PopDen.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "959472d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# code to fix FIPS_CODE column in PopDen\n",
    "\n",
    "# convert to int to get rid of decimals\n",
    "PopDen['FIPS_CODE'] = PopDen['FIPS_CODE'].astype(int)\n",
    "\n",
    "# pad additional zeroes\n",
    "PopDen['FIPS_CODE'] = PopDen['FIPS_CODE'].astype(str)\n",
    "PopDen['FIPS_CODE'] = PopDen['FIPS_CODE'].str.zfill(5)\n",
    "\n",
    "# code to fix FIPS CODE column in HomePrice\n",
    "\n",
    "# pad additional zeroes\n",
    "HomePrice['FIPS code'] = HomePrice['FIPS code'].astype(str)\n",
    "HomePrice['FIPS code'] = HomePrice['FIPS code'].str.zfill(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1a77cb8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "PopDen.head(20).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84cf668c",
   "metadata": {},
   "outputs": [],
   "source": [
    "HomePrice.head(20).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "360ae852",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14ca9a30",
   "metadata": {},
   "outputs": [],
   "source": [
    "# merge the population density data to the main dataframe\n",
    "df = df.merge(PopDen['B01001_calc_PopDensity'], how = 'left',\n",
    "                left_on = 'ST_CT_FIPS', right_on = PopDen['FIPS_CODE'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ccb52f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# using pd.concat\n",
    "# merge the population density data to the main dataframe\n",
    "df = pd.concat([df, PopDen[['B01001_calc_PopDensity', 'Population']]],\n",
    "                  keys = ['ST_CT_FIPS', 'FIPS_CODE'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae32f16c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df.head(30).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47236998",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Population'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e6c937b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# merge the home price index data to the main dataframe\n",
    "df = pd.merge(df, HomePrice,  how='left', left_on=['ST_CT_FIPS','YEAR'], right_on = ['FIPS code','Year'])\n",
    "df.drop(['HPI with 2000 base', 'HPI with 1990 base', 'Annual Change (%)', 'Year', 'FIPS code', 'County', 'State',], axis=1, inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5b0a44f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head(30).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba0032d3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83d2d8c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad89c6ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afe271e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# code to replace \".\" so that we can ultimately turn this into a number\n",
    "new_list = []\n",
    "for i in df['HPI']:\n",
    "    a_string = str(i)\n",
    "    new_string = a_string.translate(str.maketrans('', '', string.punctuation))\n",
    "    new_list.append(new_string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "039f568c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# fix newHPI column\n",
    "df['newHPI'] = pd.DataFrame(new_list)\n",
    "df['newHPI'] = df['newHPI'].replace('nan', 'NaN')\n",
    "df['newHPI'] = df['newHPI'].replace('', 'NaN')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af06f0a4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0ae577c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "66275a93",
   "metadata": {},
   "source": [
    "resources"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "050aad81",
   "metadata": {},
   "source": [
    "CZ FIPS documentation: https://www.irsa.miami.edu/_assets/pdf/Documents/fips_statecounty_code.pdf\n",
    "\n",
    "Population density: https://covid19.census.gov/datasets/21843f238cbb46b08615fc53e19e0daf_1/explore?location=2.632620%2C0.315550%2C1.00\n",
    "\n",
    "Home price index: https://www.fhfa.gov/DataTools/Downloads/Pages/House-Price-Index-Datasets.aspx\n",
    "\n",
    "maybe useful: https://www.nar.realtor/research-and-statistics/housing-statistics/county-median-home-prices-and-monthly-mortgage-payment\n",
    "        \n",
    "land values: https://www.nass.usda.gov/Publications/Todays_Reports/reports/land0822.pdf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24d2a028",
   "metadata": {},
   "source": [
    "data that I created:\n",
    " - concatenated state and county codes for indentification\n",
    " - storm duration\n",
    " - storm area\n",
    " - county population density (pulled from other dataset)\n",
    " - land values (pulled from other dataset)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
